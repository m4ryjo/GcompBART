gc_softbart_probit <- function(X, Y, X_test = NULL, num_tree = 20,
                               k = 1, hypers = NULL, opts = NULL, verbose = TRUE) {
  # Ensure binary outcome
  if (is.factor(Y)) {
    stopifnot(length(levels(Y)) == 2)
    Y <- as.numeric(Y) - 1
  }

  stopifnot(length(table(Y)) == 2)

  # Offset for probit link
  pnorm_offset <- mean(Y)
  offset <- qnorm(pnorm_offset)

  # Set up hypers
  if (is.null(hypers)) {
    hypers <- Hypers(X = X, Y = Y)
  } else {
    hypers$X <- X
    hypers$Y <- Y
  }

  hypers$sigma_mu <- 3 / k / sqrt(num_tree)
  hypers$sigma <- 1
  hypers$sigma_hat <- 1
  hypers$num_tree <- num_tree
  hypers$group <- (1:ncol(X) - 1)

  # Set up opts
  if (is.null(opts)) {
    opts <- Opts()
  }
  opts$update_sigma <- FALSE
  opts$num_print <- 2147483647

  # Normalization
  make_01_norm <- function(x) {
    a <- min(x)
    b <- max(x)
    function(y) (y - a) / (b - a)
  }

  ecdfs <- vector("list", ncol(X))
  for (i in seq_along(ecdfs)) {
    xi <- X[, i]
    if (length(unique(xi)) == 1) {
      ecdfs[[i]] <- identity
    } else if (length(unique(xi)) == 2) {
      ecdfs[[i]] <- make_01_norm(xi)
    } else {
      ecdfs[[i]] <- ecdf(xi)
    }
  }

  for (i in seq_along(ecdfs)) {
    X[, i] <- ecdfs[[i]](X[, i])
    if (!is.null(X_test)) {
      X_test[, i] <- ecdfs[[i]](X_test[, i])
    }
  }

  # Make forest
  probit_forest <- MakeForest(hypers, opts, FALSE)

  # Initialize Z
  mu <- as.numeric(probit_forest$do_predict(X))
  lower <- ifelse(Y == 0, -Inf, 0)
  upper <- ifelse(Y == 0, 0, Inf)

  # Accumulate mean predictions
  mu_train_sum <- rep(0, length(Y))
  mu_test_sum <- if (!is.null(X_test)) rep(0, nrow(X_test)) else NULL

  # Warmup
  for (i in 1:opts$num_burn) {
    Z <- rtruncnorm(n = length(Y), a = lower, b = upper, mean = mu + offset, sd = 1)
    mu <- probit_forest$do_gibbs(X, Z - offset, X, 1)
  }

  # Save
  for (i in 1:opts$num_save) {
    for (j in 1:opts$num_thin) {
      Z <- rtruncnorm(n = length(Y), a = lower, b = upper, mean = offset + mu, sd = 1)
      mu <- probit_forest$do_gibbs(X, Z - offset, X, 1)
    }

    mu_train_sum <- mu_train_sum + mu
    if (!is.null(X_test)) {
      mu_test_sum <- mu_test_sum + probit_forest$do_predict(X_test)
    }
  }

  mu_train_mean <- mu_train_sum / opts$num_save
  mu_test_mean <- if (!is.null(mu_test_sum)) mu_test_sum / opts$num_save else NULL

  # Return minimal output
  out <- list(
    mu_train_mean = mu_train_mean + offset,
    mu_test_mean = if (!is.null(mu_test_mean)) mu_test_mean + offset else NULL,
    offset = offset,
    ecdfs = ecdfs,
    opts = opts,
    forest = probit_forest
  )

  class(out) <- "softbart_probit"
  return(out)
}

